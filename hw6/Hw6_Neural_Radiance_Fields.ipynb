{"cells":[{"cell_type":"markdown","metadata":{"id":"9Z3xs9PlnzCX"},"source":["# **W24 EECS 442 HW6: Neural Radiance Fields**\n","\n","__Please provide the following information__\n","\n","[Your first name] [Your last name], [Your UMich uniqname]"]},{"cell_type":"markdown","source":["## Brief Overview\n","\n","In this problem set, you will implement Neural Radiance Fields (NeRF).\n","\n","A NeRF is an MLP that can generate novel views 3D scenes. It is trained on a set of 2D images with camera poses. The network takes viewing direction and spatial location (5D vector) as input and predicts opacity and RGB color (4D vector), and the novel views are rendered using volume rendering.\n","\n"],"metadata":{"id":"mqYCtrFiSePN"}},{"cell_type":"markdown","metadata":{"id":"bONWuwTInzCZ"},"source":["## Task 4 Starting"]},{"cell_type":"code","source":["# ignore this if you are not running on colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1SmS-7_QZydQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710190943034,"user_tz":240,"elapsed":21211,"user":{"displayName":"Yuhang Ning","userId":"01700702461061293240"}},"outputId":"f5a9ac66-9727-4f24-d222-36f0f9bb0698"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67_c5zMWnzCZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710190957790,"user_tz":240,"elapsed":13509,"user":{"displayName":"Yuhang Ning","userId":"01700702461061293240"}},"outputId":"0f6c1e89-a918-48e6-b41b-5e3405012584"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.4.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (67.7.2)\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"]}],"source":["!pip install imageio-ffmpeg\n","!pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTZB_snanzCa"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import random\n","from tqdm.notebook import tqdm\n","import os, imageio\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms.functional as TF\n","from torchsummary import summary\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(\"Using the GPU!\")\n","else:\n","    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"]},{"cell_type":"markdown","metadata":{"id":"6md9X0oGnzCb"},"source":["## Task 4 Implementation of NeRF\n","\n","In Task4, we will implement the NeRF from scratch and build the training pipeline.\n","\n","We will first generate rays based on camera poses, query the network with positions and directions, and finally use volume rendering to obtain the RGB output.\n","\n","Fuctions to implement:\n","\n","1.   Function ${\\tt positional\\_encoder(x, L\\_embed=6)}$\n","2.   Calculation of the 3D points sampled along the rays\n","2.   Calculation of ${\\tt rgb\\_map}$, ${\\tt depth\\_map}$\n","1.   Function ${\\tt train}$"]},{"cell_type":"markdown","source":["### Setup data\n","\n","To minimize training time, we will use a tiny dataset the only contains 106 images and their camera poses."],"metadata":{"id":"2tTJlVKjnnap"}},{"cell_type":"code","source":["%%capture\n","!wget -O tiny_nerf_data.npz 'https://drive.google.com/uc?export=download&id=1gt6PXUo9DQgZUEhw-CZJHqr7l0uGO-db'"],"metadata":{"id":"iVDJy9JwOv8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rawData = np.load(\"tiny_nerf_data.npz\",allow_pickle=True)\n","images = rawData[\"images\"]\n","poses = rawData[\"poses\"]\n","focal = rawData[\"focal\"]\n","H, W = images.shape[1:3]\n","H = int(H)\n","W = int(W)\n","print(\"Images: {}\".format(images.shape))\n","print(\"Camera Poses: {}\".format(poses.shape))\n","print(\"Focal Length: {:.4f}\".format(focal))\n","\n","testimg, testpose = images[99], poses[99]\n","plt.imshow(testimg)\n","plt.title('Dataset example')\n","plt.show()\n","images = torch.Tensor(images).to(device)\n","poses = torch.Tensor(poses).to(device)\n","testimg = torch.Tensor(testimg).to(device)\n","testpose = torch.Tensor(testpose).to(device)"],"metadata":{"id":"vPkNafKsOHZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate Rays\n","\n","Ray generation is a core component of NeRF. We will first implement a function that generates the camera rays for each pixel.\n","\n","Each ray includes two components: 1) origin and 2) direction."],"metadata":{"id":"VUIKCIqXoyVm"}},{"cell_type":"code","source":["def get_rays(H, W, focal, pose):\n","  \"\"\"\n","  This function generates camera rays for each pixel in an image. It calculates the origin and direction of rays\n","  based on the camera's intrinsic parameters (focal length) and extrinsic parameters (pose).\n","  The rays are generated in world coordinates, which is crucial for the NeRF rendering process.\n","\n","  Parameters:\n","  H (int): Height of the image in pixels.\n","  W (int): Width of the image in pixels.\n","  focal (float): Focal length of the camera.\n","  pose (torch.Tensor): Camera pose matrix of size 4x4.\n","\n","  Returns:\n","  tuple: A tuple containing two elements:\n","      rays_o (torch.Tensor): Origins of the rays in world coordinates.\n","      rays_d (torch.Tensor): Directions of the rays in world coordinates.\n","  \"\"\"\n","  # Create a meshgrid of image coordinates (i, j) for each pixel in the image.\n","  i, j = torch.meshgrid(\n","      torch.arange(W, dtype=torch.float32),\n","      torch.arange(H, dtype=torch.float32)\n","  )\n","  i = i.t()\n","  j = j.t()\n","\n","  # Calculate the direction vectors for each ray originating from the camera center.\n","  # We assume the camera looks towards -z.\n","  # The coordinates are normalized with respect to the focal length.\n","  dirs = torch.stack(\n","      [(i - W * 0.5) / focal,\n","        -(j - H * 0.5) / focal,\n","        -torch.ones_like(i)], -1\n","      ).to(device)\n","\n","  # Transform the direction vectors (dirs) from camera coordinates to world coordinates.\n","  # This is done using the rotation part (first 3 columns) of the pose matrix.\n","  rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1)\n","\n","  # The ray origins (rays_o) are set to the camera position, given by the translation part (last column) of the pose matrix.\n","  # The position is expanded to match the shape of rays_d for broadcasting.\n","  rays_o = pose[:3, -1].expand(rays_d.shape)\n","\n","  # Return the origins and directions of the rays.\n","  return rays_o, rays_d"],"metadata":{"id":"SfVEBqg7PpGn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Positional Encoding\n","\n","In the NeRF we leverage Fourier positional encoding to model the high frequencies.\n","\n","Given an input $x$, we will encode it into $\\gamma(x)=(x,\\text{sin}(2^{0} x),\\text{cos}(2^{0} x), \\text{sin}(2^{1} x), \\text{cos}(2^{1} x), \\cdots, \\text{sin}(2^{L\\_embed-1} x), (2^{L\\_embed-1} x))$."],"metadata":{"id":"4Nwstzdaq0_K"}},{"cell_type":"code","source":["def positional_encoder(x, L_embed=6):\n","  \"\"\"\n","  This function applies positional encoding to the input tensor. Positional encoding is used in NeRF\n","  to allow the model to learn high-frequency details more effectively. It applies sinusoidal functions\n","  at different frequencies to the input.\n","\n","  Parameters:\n","  x (torch.Tensor): The input tensor to be positionally encoded.\n","  L_embed (int): The number of frequency levels to use in the encoding. Defaults to 6.\n","\n","  Returns:\n","  torch.Tensor: The positionally encoded tensor.\n","  \"\"\"\n","\n","  # Initialize a list with the input tensor.\n","  rets = [x]\n","\n","  # Loop over the number of frequency levels.\n","  for i in range(L_embed):\n","    #############################################################################\n","    #                                   TODO                                    #\n","    #############################################################################\n","    pass\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","\n","\n","  # Concatenate the original and encoded features along the last dimension.\n","  return torch.cat(rets, -1)"],"metadata":{"id":"x1E7EAtdpdda"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cumulative Product\n","\n","The RGB value of each pixel is computed by a weighted sum of the points along the ray. Higher weight indicates higher opacity, which means the further points will be occluded. The cumulative product fuction ensures this rendering procedure."],"metadata":{"id":"15lLqmVRsJ1w"}},{"cell_type":"code","source":["def cumprod_exclusive(tensor: torch.Tensor):\n","  \"\"\"\n","  Compute the exclusive cumulative product of a tensor along its last dimension.\n","  'Exclusive' means that the cumulative product at each element does not include the element itself.\n","  This function is used in volume rendering to compute the product of probabilities\n","  along a ray, excluding the current sample point.\n","\n","  Parameters:\n","  tensor (torch.Tensor): The input tensor for which to calculate the exclusive cumulative product.\n","\n","  Returns:\n","  torch.Tensor: The tensor after applying the exclusive cumulative product.\n","  \"\"\"\n","\n","  # Compute the cumulative product along the last dimension of the tensor.\n","  cumprod = torch.cumprod(tensor, -1)\n","\n","  # Roll the elements along the last dimension by one position.\n","  # This shifts the cumulative products to make them exclusive.\n","  cumprod = torch.roll(cumprod, 1, -1)\n","\n","  # Set the first element of the last dimension to 1, as the exclusive product of the first element is always 1.\n","  cumprod[..., 0] = 1.\n","\n","  return cumprod"],"metadata":{"id":"9Zjm6C0Yppwm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### NeRF Model\n","\n","The main model of NeRF will be implemented here. To reduce training time, we will only use a very small MLP model as our network. The MLP takes the output of positional encoder as input and predicts the opacity and RGB value."],"metadata":{"id":"VUP1WrWct0YV"}},{"cell_type":"code","source":["class VeryTinyNerfModel(torch.nn.Module):\n","  \"\"\"\n","  A very small implementation of a Neural Radiance Field (NeRF) model. This model is a simplified\n","  version of the standard NeRF architecture, it consists of a simple feedforward neural network with three linear layers.\n","\n","  Parameters:\n","  filter_size (int): The number of neurons in the hidden layers. Default is 128.\n","  num_encoding_functions (int): The number of sinusoidal encoding functions. Default is 6.\n","  \"\"\"\n","\n","  def __init__(self, filter_size=128, num_encoding_functions=6):\n","    super(VeryTinyNerfModel, self).__init__()\n","    self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n","    self.layer2 = torch.nn.Linear(filter_size, filter_size)\n","    self.layer3 = torch.nn.Linear(filter_size, 4)\n","    self.relu = torch.nn.functional.relu\n","\n","  def forward(self, x):\n","    x = self.relu(self.layer1(x))\n","    x = self.relu(self.layer2(x))\n","    x = self.layer3(x)\n","    return x"],"metadata":{"id":"PYMcW4ESQLdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Volume Rendering\n","\n","After the implementation of ray generation and MLP model, we can now implement the volume rendering, which is the major procedure of NeRF."],"metadata":{"id":"TQWdpm_bzR33"}},{"cell_type":"code","source":["def render(model, rays_o, rays_d, near, far, n_samples, rand=False):\n","  \"\"\"\n","  Render a scene using a Neural Radiance Field (NeRF) model. This function samples points along rays,\n","  evaluates the NeRF model at these points, and applies volume rendering techniques to produce an image.\n","\n","  Parameters:\n","  model (torch.nn.Module): The NeRF model to be used for rendering.\n","  rays_o (torch.Tensor): Origins of the rays.\n","  rays_d (torch.Tensor): Directions of the rays.\n","  near (float): Near bound for depth sampling along the rays.\n","  far (float): Far bound for depth sampling along the rays.\n","  n_samples (int): Number of samples to take along each ray.\n","  rand (bool): If True, randomize sample depths. Default is False.\n","\n","  Returns:\n","  tuple: A tuple containing the RGB map and depth map of the rendered scene.\n","  \"\"\"\n","\n","  # Sample points along each ray, from 'near' to 'far'.\n","  z = torch.linspace(near, far, n_samples).to(device)\n","  if rand:\n","    mids = 0.5 * (z[..., 1:] + z[..., :-1])\n","    upper = torch.cat([mids, z[..., -1:]], -1)\n","    lower = torch.cat([z[..., :1], mids], -1)\n","    t_rand = torch.rand(z.shape).to(device)\n","    z = lower + (upper - lower) * t_rand\n","\n","  #############################################################################\n","  #                                   TODO                                    #\n","  #############################################################################\n","  # Compute 3D coordinates of the sampled points along the rays.\n","  points = None\n","  #############################################################################\n","  #                             END OF YOUR CODE                              #\n","  #############################################################################\n","\n","  # Flatten the points and apply positional encoding.\n","  flat_points = torch.reshape(points, [-1, points.shape[-1]])\n","  flat_points = positional_encoder(flat_points)\n","\n","  # Evaluate the model on the encoded points in chunks to manage memory usage.\n","  chunk = 1024 * 32\n","  raw = torch.cat([model(flat_points[i:i + chunk]) for i in range(0, flat_points.shape[0], chunk)], 0)\n","  raw = torch.reshape(raw, list(points.shape[:-1]) + [4])\n","\n","  # Compute densities (sigmas) and RGB values from the model's output.\n","  sigma = F.relu(raw[..., 3])\n","  rgb = torch.sigmoid(raw[..., :3])\n","\n","  # Perform volume rendering to obtain the weights of each point.\n","  one_e_10 = torch.tensor([1e10], dtype=rays_o.dtype).to(device)\n","  dists = torch.cat((z[..., 1:] - z[..., :-1], one_e_10.expand(z[..., :1].shape)), dim=-1)\n","  alpha = 1. - torch.exp(-sigma * dists)\n","  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n","\n","  #############################################################################\n","  #                                   TODO                                    #\n","  #############################################################################\n","  # Compute the weighted sum of RGB values along each ray to get the final pixel color.\n","  rgb_map = None\n","\n","  # Compute the depth map as the weighted sum of sampled depths.\n","  depth_map = None\n","  #############################################################################\n","  #                             END OF YOUR CODE                              #\n","  #############################################################################\n","\n","  return rgb_map, depth_map"],"metadata":{"id":"X1Lbjn0EqZba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Testing your rendering\n","Here we provided you with correct pretrained weight and the inference code. Please run this to test your implementation for rendering!\n","\n","The output image might be a bit blury but that's fine due to the limited data size used to train the model."],"metadata":{"id":"pyoLas0OJ0UZ"}},{"cell_type":"code","source":["!pip install gdown\n","import gdown\n","\n","url = 'https://drive.google.com/uc?id=1Mj9A7f4BsbPLw2cUGcUgCkDArTJz9q_h'\n","output = 'pretrained.pth'\n","gdown.download(url, output, quiet=False)"],"metadata":{"id":"RZoHxS_pJz5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nerf = VeryTinyNerfModel()\n","nerf = nn.DataParallel(nerf).to(device)\n","ckpt = torch.load('pretrained.pth')\n","nerf.load_state_dict(ckpt)\n","test_img_idx_list = [0, 40, 80]\n","H, W = 100, 100\n","with torch.no_grad():\n","  for test_img_idx in test_img_idx_list:\n","    rays_o, rays_d = get_rays(H, W, focal, poses[test_img_idx])\n","    # TODO: add your own function call to render\n","\n","    #\n","    plt.figure(figsize=(9,3))\n","\n","    plt.subplot(131)\n","    picture = rgb.cpu()\n","    plt.title(\"RGB Prediction #{}\".format(test_img_idx))\n","    plt.imshow(picture)\n","\n","    plt.subplot(132)\n","    picture = depth.cpu() * (rgb.cpu().mean(-1)>1e-2)\n","    plt.imshow(picture, cmap='gray')\n","    plt.title(\"Depth Prediction #{}\".format(test_img_idx))\n","\n","    plt.subplot(133)\n","    plt.title(\"Ground Truth #{}\".format(test_img_idx))\n","    plt.imshow(rawData[\"images\"][test_img_idx])\n","    plt.show()"],"metadata":{"id":"0i7uJJu-JwZF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training Loop\n","\n","In the training loop, the model is trained to fit one image randomly picked from the dataset at each iteration."],"metadata":{"id":"8RRNA8b5z0ql"}},{"cell_type":"code","source":["mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.])).to(device)\n","\n","def train(model, optimizer, n_iters=3000):\n","  \"\"\"\n","  Train the Neural Radiance Field (NeRF) model. This function performs training over a specified number of iterations,\n","  updating the model parameters to minimize the difference between rendered and actual images.\n","\n","  Parameters:\n","  model (torch.nn.Module): The NeRF model to be trained.\n","  optimizer (torch.optim.Optimizer): The optimizer used for training the model.\n","  n_iters (int): The number of iterations to train the model. Default is 3000.\n","  \"\"\"\n","\n","  psnrs = []\n","  iternums = []\n","\n","  plot_step = 500\n","  n_samples = 64   # Number of samples along each ray.\n","\n","  for i in tqdm(range(n_iters)):\n","    # Randomly select an image from the dataset and use it as the target for training.\n","    images_idx = np.random.randint(images.shape[0])\n","    target = images[images_idx]\n","    pose = poses[images_idx]\n","\n","\n","    #############################################################################\n","    #                                   TODO                                    #\n","    #############################################################################\n","    # Perform training. Use mse loss for loss calculation and update the model parameter using the optimizer.\n","    # Hint: focal is defined as a global variable in previous section\n","\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","\n","    if i % plot_step == 0:\n","      torch.save(model.state_dict(), 'ckpt.pth')\n","      # Render a test image to evaluate the current model performance.\n","      with torch.no_grad():\n","        rays_o, rays_d = get_rays(H, W, focal, testpose)\n","        rgb, depth = render(model, rays_o, rays_d, near=2., far=6., n_samples=n_samples)\n","        loss = torch.nn.functional.mse_loss(rgb, testimg)\n","        # Calculate PSNR for the rendered image.\n","        psnr = mse2psnr(loss)\n","\n","        psnrs.append(psnr.detach().cpu().numpy())\n","        iternums.append(i)\n","\n","        # Plotting the rendered image and PSNR over iterations.\n","        plt.figure(figsize=(9, 3))\n","\n","        plt.subplot(131)\n","        picture = rgb.cpu()  # Copy the rendered image from GPU to CPU.\n","        plt.imshow(picture)\n","        plt.title(f'RGB Iter {i}')\n","\n","        plt.subplot(132)\n","        picture = depth.cpu() * (rgb.cpu().mean(-1)>1e-2)\n","        plt.imshow(picture, cmap='gray')\n","        plt.title(f'Depth Iter {i}')\n","\n","        plt.subplot(133)\n","        plt.plot(iternums, psnrs)\n","        plt.title('PSNR')\n","        plt.show()\n"],"metadata":{"id":"YdQs6VGTQIqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nerf = VeryTinyNerfModel()\n","nerf = nn.DataParallel(nerf).to(device)\n","optimizer = torch.optim.Adam(nerf.parameters(), lr=5e-3, eps = 1e-7)\n","train(nerf, optimizer)"],"metadata":{"id":"33kxNTrIQNr1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Rendering from different view points.\n","\n","Given the trained model, we can query it with different camera poses.\n","In this example, we simply pick three camera poses from the dataset and render the prediction images from them."],"metadata":{"id":"vB49SEEF1AjA"}},{"cell_type":"code","source":["nerf = VeryTinyNerfModel()\n","nerf = nn.DataParallel(nerf).to(device)\n","ckpt = torch.load('ckpt.pth')\n","nerf.load_state_dict(ckpt)\n","test_img_idx_list = [0, 40, 80]\n","H, W = 100, 100\n","with torch.no_grad():\n","  for test_img_idx in test_img_idx_list:\n","    rays_o, rays_d = get_rays(H, W, focal, poses[test_img_idx])\n","    ## TODO: add your own function call to render\n","\n","    #\n","    plt.figure(figsize=(9,3))\n","\n","    plt.subplot(131)\n","    picture = rgb.cpu()\n","    plt.title(\"RGB Prediction #{}\".format(test_img_idx))\n","    plt.imshow(picture)\n","\n","    plt.subplot(132)\n","    picture = depth.cpu() * (rgb.cpu().mean(-1)>1e-2)\n","    plt.imshow(picture, cmap='gray')\n","    plt.title(\"Depth Prediction #{}\".format(test_img_idx))\n","\n","    plt.subplot(133)\n","    plt.title(\"Ground Truth #{}\".format(test_img_idx))\n","    plt.imshow(rawData[\"images\"][test_img_idx])\n","    plt.show()"],"metadata":{"id":"TamsArO0b_1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate pdf\n","# %%capture\n","get_ipython().system(\"pip install pypandoc\")\n","get_ipython().system(\"apt-get install texlive texlive-xetex texlive-latex-extra pandoc\")\n","drive_mount_point = \"/content/drive/\"\n","gdrive_home = os.path.join(drive_mount_point, \"My Drive/\")\n","notebookpath=\"/content/drive/MyDrive/Colab Notebooks/\"\n","# change the name to your ipynb file name shown on the top left of Colab window\n","# Important: make sure that your file name does not contain spaces!\n","file_name=\"\"\n","get_ipython().system(\n","  f\"jupyter nbconvert --output-dir='{gdrive_home}' '{notebookpath}''{file_name}' --to pdf\"\n",")"],"metadata":{"id":"ABmZjsbPHKcn"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.16"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}