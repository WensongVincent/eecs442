{"cells":[{"cell_type":"markdown","metadata":{"id":"sJoZaCETrMUp"},"source":["## Homework 5: Diffusion Models"]},{"cell_type":"markdown","metadata":{"id":"FoFqD-rh5tvx"},"source":["### Run the following code to setup the necessary requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21133,"status":"ok","timestamp":1711420747775,"user":{"displayName":"Jinfan Zhou","userId":"12479551648996803770"},"user_tz":240},"id":"znW9wWXgrZDy","outputId":"335bd4f2-7bb3-4974-90a8-0e1eae1a474b"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huhMrHOdrl__"},"outputs":[],"source":["# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a EECS442 folder and put all the files under HW5 folder, then 'EECS442/HW5'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'EECS442/HW5'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-eurB5ptadz"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_g8mdaPytl-G"},"outputs":[],"source":["import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","sys.path.append(GOOGLE_DRIVE_PATH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQfqllpsu5aU"},"outputs":[],"source":["print(GOOGLE_DRIVE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"4hztz_3EoDtZ"},"source":["You need to change your working directory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJQE8p-cuw7u"},"outputs":[],"source":["%cd /content/drive/MyDrive/EECS442/HW5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ucsQwPirNzm"},"outputs":[],"source":["!pip install certifi>=2022.9.14\n","!pip install charset-normalizer>=2.1.1\n","!pip install contourpy>=1.0.5\n","!pip install cycler>=0.11.0\n","!pip install fonttools>=4.37.2\n","!pip install idna>=3.4\n","!pip install kiwisolver>=1.4.4\n","!pip install matplotlib>=3.6.0\n","!pip install numpy>=1.23.3\n","!pip install packaging>=21.3\n","!pip install Pillow>=9.2.0\n","!pip install pyparsing>=3.0.9\n","!pip install python-dateutil>=2.8.2\n","!pip install PyYAML>=6.0\n","!pip install requests>=2.28.1\n","!pip install scipy>=1.9.1\n","!pip install six>=1.16.0\n","!pip install tqdm>=4.64.1\n","!pip install typing-extensions>=4.3.0\n","!pip install urllib3>=1.26.12\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ou14Euqpw3-W"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"0J3HNC8orMUt"},"source":["## Task 1: Unconditional Sampling with DDPM"]},{"cell_type":"markdown","metadata":{"id":"QAAUzhCo44hu"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAwLV3_X459y"},"outputs":[],"source":["from functools import partial\n","import os\n","import argparse\n","import yaml\n","\n","import torch\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","from util.logger import get_logger\n","# GPUs are preferred\n","logger = get_logger()\n","device_str = f\"cuda:0\" if torch.cuda.is_available() else 'cpu'\n","logger.info(f\"Device set to {device_str}.\")\n","device = torch.device(device_str)\n","\n","# set output directory\n","save_dir = 'results'\n","ddpm_out_path = os.path.join(save_dir, 'uncond_ddpm')\n","os.makedirs(ddpm_out_path, exist_ok=True)\n","for img_dir in ['input', 'output', 'progress']:\n","    os.makedirs(os.path.join(ddpm_out_path, img_dir), exist_ok=True)\n"]},{"cell_type":"markdown","metadata":{"id":"1OR9ez37rMUt"},"source":["In this task, you will implement the sampling Algorithm proposed in Denoising Diffusion Probabilistic Models(DDPM) paper as shown below:\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1Nz3QHfWxdTHcl0NkJt0BsUUTj0RCx7o9)\n"]},{"cell_type":"markdown","metadata":{"id":"WEHxDY-IAjRU"},"source":["(1) Now let's implement the variance schedule. As you can see in the DDPM sampling algorithm, We will need $\\alpha_t$ for each timestep $t$. $\\alpha_t$ is a notaion for $1-\\beta_t$, where $\\beta_t$ is the true variance that increases from $t=1$ to $t=T$. There are many different variance shedules such as linear schedule and cosine schedule. Follow the instruction in `guided_diffusion/simple_diffusion.py` to implement `get_named_beta_schedule()`.\n"]},{"cell_type":"markdown","metadata":{"id":"AVb9EZYPVe5P"},"source":["Cosine schedule is proposed by [iDDPM](https://arxiv.org/pdf/2102.09672.pdf). You can find the detailed motivation in the paper. The calulation of $\\beta$ depends on $\\alpha$, the cumulated product of $\\alpha$ is defined as $$\\bar{\\alpha}_t=\\frac{f(t)}{f(0)}$$,\n","where\n","$$f(t)=\\cos\\left({\\frac{t/T+s}{1+s}}\\cdot{\\frac{\\pi}{2}}\\right)^{2}$$\n","We use small $s=0.008$ such that $\\sqrt{\\beta_0}$ was slightly smaller than the pixel\n","bin size 1/127.5.\n","According to the definition of $\\alpha_t$, we can then get $\\beta_t$ as\n","$$\\beta_t=1-\\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}}$$\n","\n","Also, clip $\\beta_t$ to be no larger than\n","0.999 to prevent singularities at the end of the diffusion process.\n"]},{"cell_type":"markdown","metadata":{"id":"BCD0vNAHbXgU"},"source":["Run the following code to see your output."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bAUx86OWAizG"},"outputs":[],"source":["from guided_diffusion.simple_diffusion import get_named_beta_schedule\n","import numpy as np\n","\n","num_steps = 1000\n","schedule_name = 'cosine'\n","\n","print(get_named_beta_schedule(schedule_name, num_steps))"]},{"cell_type":"markdown","metadata":{"id":"uR1rWGdeo_Vs"},"source":["(2) Now you have implemented your variance schedule $\\{\\beta_1, \\beta_2, ..., \\beta_T\\}$. In practice, we use $\\alpha_t$ and accumulated product ${\\overline{\\alpha}}_t = \\prod_{i=1}^t \\alpha_i$. **Follow the instruction to complete `__init__()` of `DDPMDiffusion`** to compute the needed values that will be used during the sampling process. They only need to be calculated once during initialzation and we can directly access them later during sampling.\n","\n","Hint: You can use `np.cumprod()` to calculate the cumulated product."]},{"cell_type":"markdown","metadata":{"id":"n6gioR7vTKZB"},"source":["Let's now create the model.\n","\n","Before you run the code below, make sure that you have downloaded the pretrained model\n","[ffhq_10M.pt](https://drive.google.com/drive/folders/1jElnRoFv7b31fG0v6pTSQkelbSX3xGZh?usp=sharing) and put it under `model` directory."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1MAG6auuTX4h"},"outputs":[],"source":["from guided_diffusion.unet import create_model\n","from data.dataloader import get_dataset, get_dataloader\n","\n","# Here is the model configuration of the Pretrained UNet model that we will be using.\n","# This configuration should be consistent with the pretrained model, so you don't need to modify them.\n","# You can find the detailed definition of the UNet in guided_diffusion/unet.py\n","\n","model_config = {\n","    'image_size': 256,\n","    'num_channels': 128,\n","    'num_res_blocks': 1,\n","    'channel_mult': \"\",\n","    'learn_sigma': True,\n","    'class_cond': False,\n","    'use_checkpoint': False,\n","    'attention_resolutions': 16,\n","    'num_heads': 4,\n","    'num_head_channels': 64,\n","    'num_heads_upsample': -1,\n","    'use_scale_shift_norm': True,\n","    'dropout': 0.0,\n","    'resblock_updown': True,\n","    'use_fp16': False,\n","    'use_new_attention_order': False,\n","    'model_path': 'models/ffhq_10m.pt'\n","}\n","\n","# Load model\n","ddpm_beta = get_named_beta_schedule('linear', 1000)\n","\n","model = create_model(betas=ddpm_beta, **model_config)\n","model = model.to(device)\n","model.eval()  # Set the model to the evaluation mode as we don't need to train it.\n"]},{"cell_type":"markdown","metadata":{"id":"Dc3hCAyk856d"},"source":["(3) Now we need to implement the posterior sampling of DDPM as shown in line 4 in Algorithm 2. As you can notice that in the original sampling algorithm of DDPM, the model is trained to predict the noise $\\epsilon$.\n","\n","Our pretrained model takes $x_t$ and $t$ as input and predict the noise $\\epsilon$.\n","\n","What's more, our model is also trained to predict the variance $\\sigma_t$. The model output is a torch tensor of shape $(B, C, H, W)$, where $B$ is the batch size, $C$ is the number of channels and $H$, $W$ are the height and width respectively. $C$ here for our model is 6, with the first 3 channels for the noise prediction $\\epsilon$ and the last 3 chennels for $\\sigma_t$.\n","\n","Implement the `p_sample` function of `DDPMDiffusion` in `guided_diffusion/simple_diffusion.py` for unconditional posterior sampling. Follow the sampling algorithm. Attach your code to the report."]},{"cell_type":"markdown","metadata":{"id":"lLbVbSbNVHx_"},"source":["(4) Now we have everything we need to perform unconditional sampling!\n","\n","Implement the `p_sample_loop` of `DDPMDiffusion` in `simple_diffusion.py` for unconditional sampling, using the DDPM sampling algorithm."]},{"cell_type":"markdown","metadata":{"id":"zon0N9m5X2sl"},"source":["(6) Run the code below to see what we can get from unconditional distillation. Include your results in your report."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qkzea18kbHN3"},"outputs":[],"source":["from guided_diffusion.simple_diffusion import *\n","import torchvision\n","from util.img_utils import clear_color, mask_generator\n","from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n","from util.img_utils import clear_color, mask_generator\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JctHDQR0YYnj"},"outputs":[],"source":["diffusion_config = {\n","    'sampler': 'ddpm',\n","    'steps': 1000,\n","    'noise_schedule': 'linear',\n","    'model_mean_type': 'epsilon',\n","    'model_var_type': 'learned_range',\n","    'dynamic_threshold': False,\n","    'clip_denoised': True,\n","    'rescale_timesteps': False,\n","    'timestep_respacing': 1000}\n","\n","sampler = create_sampler(**diffusion_config)\n","sample_fn = partial(sampler.p_sample_loop, model=model, measurement_cond_fn=None)\n","\n","x_start = torch.randn((1, 3, 256, 256), device=device)\n","out_path = os.path.join(save_dir, 'uncond_ddpm')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAa0JHWNeyKO"},"outputs":[],"source":["sample = sample_fn(x_start=x_start, measurement=None, record=True, save_root=out_path, uncond=True)"]},{"cell_type":"markdown","metadata":{"id":"KArdW8Pp3Zv5"},"source":["We start from random noise of the same size as our output image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_v1tm0AhEwC"},"outputs":[],"source":["torchvision.transforms.functional.to_pil_image((x_start[0] + 1)/2)"]},{"cell_type":"markdown","metadata":{"id":"tQQusT9n3lX3"},"source":["We can then generate human faces with our pretrained model when given the noise input:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YYvPuOsfsX_"},"outputs":[],"source":["plt.imsave(os.path.join(out_path, 'output', '0.png'), clear_color(sample))\n","torchvision.transforms.functional.to_pil_image((sample[0] + 1)/2)"]},{"cell_type":"markdown","metadata":{"id":"ZKk8nxB79OV6"},"source":["## Task 2: Unconditional Samping with DDIM"]},{"cell_type":"markdown","metadata":{"id":"JYp5LG3vRthf"},"source":["In this task, you will implement an improved sampling algorithm from Denoising Diffusion Implicit Models(DDIM) paper. DDIM sampling applies an improved update rule to sample from $p(x_{t-1}|x_t, x_0)$. The update rule is given by\n","\n","![](https://drive.google.com/uc?id=1_a9OdUeRRb5PPDJrQhT_8QjpLBKC812U)\n","\n","\n","\n","Leveraging the above improved update rule, DDIM can be used to accelerate the sampling algorithm by only using a subset of the timesteps as before.\n","\n","In `simple_diffusion.py` update the method `p_sample` under the class `DDIMDiffusion`, to implement the above update rule for DDIM sampling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciKHJ_TmHqm3"},"outputs":[],"source":["timestep_spacing = 100\n","\n","diffusion_config = {\n","    'sampler': 'ddim',\n","    'steps': 1000,\n","    'noise_schedule': 'linear',\n","    'model_mean_type': 'epsilon',\n","    'model_var_type': 'learned_range',\n","    'dynamic_threshold': False,\n","    'clip_denoised': True,\n","    'rescale_timesteps': True,\n","    'timestep_respacing': f'ddim{timestep_spacing}'}\n","\n","\n","sampler = create_sampler(**diffusion_config)\n","sample_fn = partial(sampler.p_sample_loop, model=model, measurement_cond_fn=None)\n","\n","x_start = torch.randn((1, 3, 256, 256), device=device)\n","out_path = os.path.join(save_dir, 'uncond_ddim')\n","save_path = os.path.join(out_path, \"progress\")\n","os.makedirs(save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fo7XQjm8QQeP"},"outputs":[],"source":["sample = sample_fn(x_start=x_start, measurement=None, record=True, save_root=out_path, uncond = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLd48aBIQSQa"},"outputs":[],"source":["x_start_plot = to_pil_image((x_start[0] + 1)/2)\n","x_start_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfqHUSGjQVaf"},"outputs":[],"source":["sample_plot = to_pil_image((sample[0] + 1)/2)\n","sample_plot"]},{"cell_type":"markdown","metadata":{"id":"b_KxqmoL9BH8"},"source":["## Task 3: Inverse problem with RePaint"]},{"cell_type":"markdown","metadata":{"id":"Z7ssSmMzWN1J"},"source":["In this task, you will be applying the generative DDPM to solve an interesting problem of Image Inpainting. Image inpainting refers to filling out regions of the image that are unknown apriori. Here, we assume that a mask $m$ indicating the known region is given to us.\n","\n","Repaint Diffusion applies an update rule to the input image as shown below,\n","\n","![](https://drive.google.com/uc?id=1Z2rrGAmNTKBzEqrqyKJ7p96lOrtl8_eS)\n","\n","where the known region is sampled using\n","\n","$x^{known}_{t-1} \\sim N\\:(\\:\\bar{\\alpha}_t  x_0, \\:(1 - \\bar{\\alpha}_t)\\:I\\:)$\n","\n","and the unknown region is sampled from the diffusion model as\n","\n","$x^{unknown}_{t-1} \\sim N\\:(\\mu_{\\theta}(\\: x_t, \\:t), \\:\\Sigma_{\\theta}(\\: x_t, \\:t))$\n","\n","and the new sample $x_{t-1}$ is obtained using\n","\n","$x_{t-1} = m ⨀ x^{known}_{t-1} + (1 - m) ⨀ x^{unknown}_{t-1} $\n","\n","In `simple_diffusion.py` update the method `p_sample` under the class `Repaint`, to implement the above update rule for inpainting.\n","\n","\n","\n","The summarized algorithm is given by\n","\n","![](https://drive.google.com/uc?id=1dLxuhCUCf_pkK4HLsbrygs90PBWkJwk3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoK16NxeQWU2"},"outputs":[],"source":["repaint_beta = get_named_beta_schedule('linear', 1000)\n","\n","model_config = {\n","    'image_size': 256,\n","    'num_channels': 128,\n","    'num_res_blocks': 1,\n","    'channel_mult': \"\",\n","    'learn_sigma': True,\n","    'class_cond': False,\n","    'use_checkpoint': False,\n","    'attention_resolutions': 16,\n","    'num_heads': 4,\n","    'num_head_channels': 64,\n","    'num_heads_upsample': -1,\n","    'use_scale_shift_norm': True,\n","    'dropout': 0.0,\n","    'resblock_updown': True,\n","    'use_fp16': False,\n","    'use_new_attention_order': False,\n","    'model_path': 'models/ffhq_10m.pt'\n","}\n","\n","\n","diffusion_config = {\n","    'sampler': 'repaint',\n","    'steps': 1000,\n","    'noise_schedule': 'linear',\n","    'model_mean_type': 'epsilon',\n","    'model_var_type': 'learned_range',\n","    'dynamic_threshold': False,\n","    'clip_denoised': True,\n","    'rescale_timesteps': True,\n","    'timestep_respacing': 250}\n","\n","repaint_conf = {\n","    \"name\": \"face_example\",\n","    \"inpa_inj_sched_prev\": True,\n","    \"n_jobs\": 1,\n","    \"print_estimated_vars\": True,\n","    \"inpa_inj_sched_prev_cumnoise\": False,\n","    \"class_cond\": False,\n","    \"schedule_jump_params\":{\n","        \"t_T\": 250,\n","        \"n_sample\": 1,\n","        \"jump_length\": 10,\n","        \"jump_n_sample\": 10,\n","    }\n","}\n","\n","repaint_model = create_model(betas=repaint_beta, **model_config)\n","repaint_model = repaint_model.to(device)\n","repaint_model.eval()\n","print(\"Model Loaded\")\n","\n","gt_path = \"data/datasets/gts/face/000000.png\"\n","gt_mask_path = \"data/datasets/gt_keep_masks/face/000000.png\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cj3i5ezoQYiN"},"outputs":[],"source":["model_kwargs_keys = ['gt', 'gt_keep_mask']\n","\n","pil_gt_image = Image.open(gt_path)\n","gt_tensor = (pil_to_tensor(pil_gt_image) / 127.5 - 1.0).to(device = 'cuda')\n","\n","pil_gt_mask = Image.open(gt_mask_path)\n","gt_mask_tensor = (pil_to_tensor(pil_gt_mask) / 255.0).to(device = 'cuda')\n","\n","model_kwargs = {\n","    'gt': gt_tensor,\n","    'gt_keep_mask': gt_mask_tensor\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6VpKlBVQbkg"},"outputs":[],"source":["sampler = create_sampler(**diffusion_config)\n","sample_fn = partial(sampler.p_sample_loop, model=repaint_model, shape=(1, 3, 256, 256), conf = repaint_conf, model_kwargs = model_kwargs)\n","\n","x_start = torch.randn((1, 3, 256, 256), device=device)\n","out_path = os.path.join(save_dir, 'repaint')\n","save_path = os.path.join(out_path, \"progress\")\n","os.makedirs(save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2qcj3mfUt02"},"outputs":[],"source":["pil_gt_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8wJLfHvXosr"},"outputs":[],"source":["pil_gt_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRsRwka1QdW_"},"outputs":[],"source":["sample = sample_fn(noise=x_start, progress = True)\n","to_pil_image((sample[0] + 1)/2)"]},{"cell_type":"markdown","metadata":{"id":"c75lN4TbyuPA"},"source":["## Task 4: Inverse problem with DPS\n","\n","In this task, you will implement the sampling algorithm with posterior sampling using a pre-trained diffusion model. This is prettey much similar to the unconditional sampling algorithm except that we now are given the corrupted input. Let's implement the algorithm below.\n","![](https://drive.google.com/uc?export=view&id=1SCjAe3UjzjLLW87ftzzcTbgm6rf-_Rlb)"]},{"cell_type":"markdown","metadata":{"id":"D0dlD3blj-Nl"},"source":["The algorithm follows the same process as unconditional sampling. The only difference here is that we need to use the prior provided by the diffusion model and optimize the **image** directly by take taking the derivative in line 7.\n","\n","(1) We have already implemented the conditional sampling part of `p_sample_loop` and `p_sample` of `DPSDiffusion` in `guided_diffusion/simple_diffusion.py` for conditional posterior sampling. But we do encourage you to take a look that into that.\n","\n","(2)You will need to implment the `PosteriorSampling` in `guided_diffusion/condition_methods.py`.\n","\n","(3) Run the following code. You should report one sample(including the raw image, corrupted image input and the algorithm output) of the inpainting task.\n","\n","(4) [Optional] Play around with other task configurations and operate the algorithm to see how the results look like. Report one sample(including the raw image, corrupted image input and the algorithm output) of the following task: motion deblur, gaussain deblur and super resolution. Compare the results and discuss how the algorithm perform in each task.\n","[Hint: Change `task_config` to paly with different tasks]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dVx6fdm-OHM"},"outputs":[],"source":["from guided_diffusion.measurements import get_noise, get_operator\n","from guided_diffusion.condition_methods import get_conditioning_method\n","from util.img_utils import clear_color, mask_generator\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CX2wGKZyyvb"},"outputs":[],"source":["# Prepare dataloader\n","# data_config = task_config['data']\n","data_config = {\n","    'name': 'ffhq',\n","    'root': './data/samples/'}\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","dataset = get_dataset(**data_config, transforms=transform)\n","loader = get_dataloader(dataset, batch_size=1, num_workers=0, train=False)\n","\n","# configuration of inpainting task\n","task_config_inpainting = {'conditioning':\n","            {'method': 'ps',\n","             'params': {'scale': 0.5}},\n","        'measurement':\n","            {'operator': {'name': 'inpainting'},\n","             'mask_opt':\n","             {'mask_type': 'random',\n","              'mask_prob_range': (0.3, 0.7),\n","              'image_size': 256},\n","              'noise': {'name': 'gaussian', 'sigma': 0.05}}\n","             }\n","\n","# configuration of motion-deblur task\n","task_config_motion_deblur = {'conditioning':\n","            {'method': 'ps',\n","             'params': {'scale': 0.3}},\n","        'measurement':\n","            {'operator': {\n","                'name': 'motion_blur',\n","                'kernel_size': 61,\n","                'intensity': 0.5},\n","              'noise': {'name': 'gaussian', 'sigma': 0.05}}\n","             }\n","\n","# configuration of gaussian-deblur task\n","task_config_gaussian_deblur = {'conditioning':\n","            {'method': 'ps',\n","             'params': {'scale': 0.3}},\n","        'measurement':\n","            {'operator': {\n","                'name': 'gaussian_blur',\n","                'kernel_size': 61,\n","                'intensity': 3.0},\n","              'noise': {'name': 'gaussian', 'sigma': 0.05}}\n","             }\n","\n","# configuration of super resolution task\n","task_config_super_resolution = {'conditioning':\n","            {'method': 'ps',\n","             'params': {'scale': 0.3}},\n","        'measurement':\n","            {'operator': {\n","                'name': 'super_resolution',\n","                'in_shape': (1, 3, 256, 256),\n","                'scale_factor': 4},\n","              'noise': {'name': 'gaussian', 'sigma': 0.05}}\n","             }\n","\n","task_config = task_config_inpainting\n","\n","measure_config = task_config['measurement']\n","operator = get_operator(device=device, **measure_config['operator'])\n","noiser = get_noise(**measure_config['noise'])\n","logger.info(f\"Operation: {measure_config['operator']['name']} / Noise: {measure_config['noise']['name']}\")\n","\n","# Prepare conditioning method\n","cond_config = task_config['conditioning']\n","cond_method = get_conditioning_method(cond_config['method'], operator, noiser, **cond_config['params'])\n","measurement_cond_fn = cond_method.conditioning\n","logger.info(f\"Conditioning method : {task_config['conditioning']['method']}\")\n","\n","diffusion_config = {\n","    'sampler': 'dps',\n","    'steps': 1000,\n","    'noise_schedule': 'linear',\n","    'model_mean_type': 'epsilon',\n","    'model_var_type': 'learned_range',\n","    'dynamic_threshold': False,\n","    'clip_denoised': True,\n","    'rescale_timesteps': False,\n","    'timestep_respacing': 1000}\n","\n","sampler = create_sampler(**diffusion_config)\n","sample_fn = partial(sampler.p_sample_loop, model=model, measurement_cond_fn=measurement_cond_fn, uncond=False)\n","\n","out_path = os.path.join(save_dir, measure_config['operator']['name'])\n","os.makedirs(out_path, exist_ok=True)\n","for img_dir in ['input', 'recon', 'progress', 'label']:\n","    os.makedirs(os.path.join(out_path, img_dir), exist_ok=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ka_zRYcM_pG_"},"outputs":[],"source":["if measure_config['operator']['name'] == 'inpainting':\n","        mask_gen = mask_generator(\n","           **measure_config['mask_opt']\n","        )\n","\n","for i, ref_img in enumerate(loader):\n","        logger.info(f\"Inference for image {i}\")\n","        fname = str(i).zfill(5) + '.png'\n","        ref_img = ref_img.to(device)\n","\n","        # Exception) In case of inpainging,\n","        if measure_config['operator'] ['name'] == 'inpainting':\n","            mask = mask_gen(ref_img)\n","            mask = mask[:, 0, :, :].unsqueeze(dim=0)\n","            measurement_cond_fn = partial(cond_method.conditioning, mask=mask)\n","            sample_fn = partial(sample_fn, measurement_cond_fn=measurement_cond_fn)\n","\n","            # Forward measurement model (Ax + n)\n","            y = operator.forward(ref_img, mask=mask)\n","            y_n = noiser(y)\n","\n","        else:\n","            # Forward measurement model (Ax + n)\n","            y = operator.forward(ref_img)\n","            y_n = noiser(y)\n","\n","        # Sampling\n","        x_start = torch.randn(ref_img.shape, device=device).requires_grad_()\n","        sample = sample_fn(x_start=x_start, measurement=y_n, record=True, save_root=out_path, uncond=False)\n","\n","        plt.imsave(os.path.join(out_path, 'input', fname), clear_color(y_n))\n","        plt.imsave(os.path.join(out_path, 'label', fname), clear_color(ref_img))\n","        plt.imsave(os.path.join(out_path, 'recon', fname), clear_color(sample))\n","\n","        break"]},{"cell_type":"markdown","metadata":{"id":"pHZtErnOBwAl"},"source":["Now let's visualize some results.\n","\n","Runnig the code below to visualize the raw image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0oaTVNAB8Ro"},"outputs":[],"source":["Image.open(os.path.join(out_path, 'label', '00000.png'))"]},{"cell_type":"markdown","metadata":{"id":"7xu6B74cDIyS"},"source":["And here is the corrupted image by random masks:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGt8XEzDDMqi"},"outputs":[],"source":["Image.open(os.path.join(out_path, 'input', '00000.png'))"]},{"cell_type":"markdown","metadata":{"id":"pjategjXDPwk"},"source":["Now let's see how our algorithm works:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuOjXvMQDTdq"},"outputs":[],"source":["Image.open(os.path.join(out_path, 'recon', '00000.png'))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
